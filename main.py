# main.py

import json
import asyncio
import os
import config  # Import the configuration file

from retriever import HybridRetriever, find_category_with_gemini_rag, load_categories_from_file
from scrapers.zoommer_scraper import zommer_scraper_for_urls
from embedder import embed_all_products, save_faiss_index, load_products, search_similar

QUERIES = [
    # "áƒ¡áƒáƒ›áƒ¡áƒ£áƒœáƒ’áƒ˜áƒ¡ áƒ¡áƒáƒáƒ—áƒ˜ áƒ›áƒ˜áƒœáƒ“áƒáƒ•áƒ˜áƒ§áƒ˜áƒ“áƒ",
    "áƒšáƒ”áƒáƒ¢áƒáƒáƒ˜ áƒ›áƒ­áƒ˜áƒ áƒ“áƒ”áƒ‘áƒ, áƒ—áƒáƒœ áƒ áƒáƒ› áƒ—áƒáƒ›áƒáƒ¡áƒ¨áƒ”áƒ‘áƒ˜áƒª áƒ’áƒáƒ›áƒ˜áƒ™áƒáƒ©áƒáƒ¡",
    # "áƒáƒ˜áƒ¤áƒáƒœáƒ˜áƒ¡ áƒ“áƒáƒ›áƒ¢áƒ”áƒœáƒ˜ áƒ®áƒáƒ› áƒáƒ  áƒ’áƒáƒ¥áƒ•áƒ—?",
    # "áƒáƒáƒ•áƒ”áƒ  áƒ‘áƒáƒœáƒ™áƒ˜",
    # "áƒ•áƒ”áƒšáƒáƒ¡áƒ˜áƒáƒ”áƒ“áƒ˜"
]

async def main():
    # Use config for file path
    categories = load_categories_from_file(config.CATEGORIES_FILE)
    if not categories:
        print("âŒ Categories not loaded.")
        return

    retriever = HybridRetriever(categories)
    selected_subcategories = []
    unique_urls = set()

    for query in QUERIES:
        print(f"\nğŸ” Query: {query}")
        # Use config for retriever parameter
        retrieved = retriever.search(query, top_k=config.RETRIEVER_TOP_K)
        if not retrieved:
            print("  âŒ Nothing found by retriever.")
            continue

        gemini_result = find_category_with_gemini_rag(query, retrieved)
        try:
            parts = gemini_result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0]
            final = parts.get('json') or json.loads(parts.get('text', '{}'))

            if "error" in final:
                print(f"  âš ï¸ Gemini: {final['error']}")
            elif "subcategory_url" in final and final["subcategory_url"] not in unique_urls:
                # Use config for base URL
                full_url = config.WEBSITE_BASE_URL + final["subcategory_url"]
                print(f"  âœ… Found: {final['subcategory_name']} â†’ {full_url}")
                selected_subcategories.append({
                    "name": final["subcategory_name"],
                    "url": final["subcategory_url"]
                })
                unique_urls.add(final["subcategory_url"])
        except Exception as e:
            print(f"  âš ï¸ Failed to parse Gemini response: {e}")
            print(json.dumps(gemini_result, indent=2, ensure_ascii=False))

    if selected_subcategories:
        print("\nğŸ—¸ Starting targeted scraping...")
        await zommer_scraper_for_urls(selected_subcategories)
    else:
        print("âŒ No subcategories selected for scraping.")
        return

    # === Step 2: Embed scraped data and save to FAISS ===
    print("\nğŸ”® Starting embedding of scraped products...")
    # Use config for file path
    products = load_products(config.SCRAPED_DATA_FILE)
    vectors, metadata = embed_all_products(products)
    save_faiss_index(vectors, metadata)

    # === Step 3: Test the search ===
    print("\nğŸ” Running example semantic search...")
    example_query = "áƒ“áƒ˜áƒ“áƒ˜ áƒ‘áƒáƒ¢áƒáƒ áƒ˜áƒ˜áƒ— áƒ¡áƒ›áƒáƒ áƒ¢ áƒ¡áƒáƒáƒ—áƒ˜ NFC-áƒ˜áƒ—"
    results = search_similar(example_query)

    for r in results:
        print(f"- {r['title']} | {r['price']} áƒšáƒáƒ áƒ˜ | {r['category']}")

if __name__ == "__main__":
    asyncio.run(main())
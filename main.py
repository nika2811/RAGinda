import json
import asyncio
from retriever import HybridRetriever, find_category_with_gemini_rag, load_categories_from_file
from scrapers.zoommer_scraper import zommer_scraper_for_urls 

QUERIES = [
    "áƒ¡áƒáƒ›áƒ¡áƒ£áƒœáƒ’áƒ˜áƒ¡ áƒ¡áƒáƒáƒ—áƒ˜ áƒ›áƒ˜áƒœáƒ“áƒ áƒ•áƒ˜áƒ§áƒ˜áƒ“áƒ",
    "áƒšáƒ”áƒáƒ¢áƒáƒáƒ˜ áƒ›áƒ­áƒ˜áƒ áƒ“áƒ”áƒ‘áƒ, áƒ—áƒáƒœ áƒ áƒáƒ› áƒ—áƒáƒ›áƒáƒ¨áƒ”áƒ‘áƒ˜áƒª áƒ’áƒáƒ›áƒ˜áƒ¥áƒáƒ©áƒáƒ¡",
    "áƒáƒ˜áƒ¤áƒáƒœáƒ˜áƒ¡ áƒ“áƒáƒ›áƒ¢áƒ”áƒœáƒ˜ áƒ®áƒáƒ› áƒáƒ  áƒ’áƒáƒ¥áƒ•áƒ—?",
    "áƒáƒáƒ•áƒ”áƒ  áƒ‘áƒáƒœáƒ™áƒ˜",
    "áƒ•áƒ”áƒšáƒáƒ¡áƒ˜áƒáƒ”áƒ“áƒ˜"
]

async def main():
    categories = load_categories_from_file("categories.json")
    if not categories:
        print("âŒ Categories not loaded.")
        return

    retriever = HybridRetriever(categories)
    selected_subcategories = []
    unique_urls = set()

    for query in QUERIES:
        print(f"\nğŸ” Query: {query}")
        retrieved = retriever.search(query, top_k=3)
        if not retrieved:
            print("  âŒ Nothing found by retriever.")
            continue

        gemini_result = find_category_with_gemini_rag(query, retrieved)
        try:
            parts = gemini_result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0]
            final = parts.get('json') or json.loads(parts.get('text', '{}'))
            
            if "error" in final:
                print(f"  âš ï¸ Gemini: {final['error']}")
            elif "subcategory_url" in final and final["subcategory_url"] not in unique_urls:
                full_url = "https://zoommer.ge" + final["subcategory_url"]
                print(f"  âœ… Found: {final['subcategory_name']} â†’ {full_url}")
                selected_subcategories.append({
                    "name": final["subcategory_name"],
                    "url": final["subcategory_url"]
                })
                unique_urls.add(final["subcategory_url"])
        except Exception as e:
            print(f"  âš ï¸ Failed to parse Gemini response: {e}")
            print(json.dumps(gemini_result, indent=2, ensure_ascii=False))

    if selected_subcategories:
        print("\nğŸ•¸ï¸ Starting targeted scraping...")
        await zommer_scraper_for_urls(selected_subcategories)
    else:
        print("âŒ No subcategories selected for scraping.")

if __name__ == "__main__":
    asyncio.run(main())